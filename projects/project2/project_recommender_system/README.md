# Machine Learning Class Project 2 - Recommender System

This is the directory for the second class project of the Machine Learning course of Fall 2017 at EPFL. Out of the three given options, the Recommender System (recommending movies to users based on their ratings) has been chosen.

This file contains practical information on the project implementation and how to run it. For more detailed explanation of the it (models implemented, hyperparameter choosing, result validation...), please refer to the report (`docs/report.pdf`). 

## Project structure

The project has the following folder (and file) structure:

* `data/`. Directory containing input data for training (`data_train.csv`) and creating the submission (`sample_submission.csv`, only user and movie ids are collected, not ratings), both given by Kaggle.
* `docs/`. Directory containing the compiled report as `report.pdf`, the raw latex file `report.tex` and the plots included on it.  
* `out/`. Directory with the output generated by the scripts, which contains files following the `submission-{timestamp}.csv` pattern. This output is compliant with the required format by Kaggle (described in `data/sample-submission.csv`).
* `src/`. Actual Python code of the implementation.
	* TODO
* `src_old`. Code of various discarded implementations that perform worse than the final one.
	* TODO
* `tmp/`. Folder containing cached data files.
* `clear.sh`. Clear all execution-derived data files in `out/` and `tmp/`.
* `requirements.txt`. Standard Python-PIP requirements file with required packages and versions.

## Environment

The required environment for running the code and reproducing the results is a computer with a valid installation of Python 3. More specifically, [Python 3.6.3](https://docs.python.org/3.6/) is used.

Besides that (and the built-in Python libraries), the following packages are used and have to be installed:

* [NumPy 1.13.3](http://www.numpy.org).
* [Pandas 0.21.1](https://pandas.pydata.org)
* [Suprise (scikit-surprise) 1.0.4](http://surpriselib.com)

The best way to reproduce the complete environment would be creating a [virtualenv](https://virtualenv.pypa.io/en/stable/) using [virtualenvwrapper](https://virtualenvwrapper.readthedocs.io/en/latest/) with the following commands:

```
mkvirtualenv ml -p python3
workon ml
```

And then, inside the root folder, install the packages specified on `requirements.txt`:

```
pip install -r requirements.txt
```

## Running

All the following commands assume that the user is working inside the environment created in the previous step.

### Main program

To run the main program, the `run.py` has to be executed from the scripts folder:

```
cd src/
python run.py
```

There are no command line arguments. It will look for a `data_train.csv` file located on `../data/` for training and a `sample_submission.csv` for creating the submission. Outputs will be generated in the `../out/` folder.

The program will output the following progress updates through the standard output:

```
TODO
```

### Test

The `run.py` script has alredy the algorithm and the hyperparameters that give best result (smaller RMSE). In order to find out those variables, cross-validation has been applied.

To run cross-validation, the `validate.py` script has been used:

```
cd src/
python validate.py
```
It has the following command line options:

```
TODO
```


TODO outputs

### Data caching

Loading and pre-processing big datasets from `.csv` files is a slow task (roughly 15-20 seconds in a modern laptop), especially when developing. To solve this problem, a caching mechanism has been implemented: instead of loading raw `.csv`, the program loads `pickle`-encoded Python objects.

To do so, the main program detects if the required `.pckl` files are available in the `tmp/` folder. If so, it imports the objects into the variables used by the rest of the script. Otherwise it imports them from the `.csv` files and then dumps those objects to the `.pckl` files.
