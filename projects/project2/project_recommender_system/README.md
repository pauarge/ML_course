# Machine Learning Class Project 2 - Recommender System

This is the directory for the second class project of the Machine Learning course of Fall 2017 at EPFL. Out of the three given options, the Recommender System (recommending movies to users based on their ratings) has been chosen.

This file contains practical information on the project implementation and how to run it. For more detailed explanation of the it (models implemented, hyperparameter choosing, result validation...), please refer to the report (`docs/report.pdf`). 

## Project structure

The project has the following folder (and file) structure:

* `data/`. Directory containing input data for training (`data_train.csv`) and creating the submission (`sample_submission.csv`, only user and movie ids are collected, not ratings), both given by Kaggle.
* `docs/`. Directory containing the compiled report as `report.pdf`, the raw latex file `report.tex` and the plots included on it.  
* `out/`. Directory with the output generated by the scripts, which contains files following the `submission-{timestamp}.csv` pattern. This output is compliant with the required format by Kaggle (described in `data/sample-submission.csv`).
* `src/`. Actual Python code of the implementation.
	* TODO
* `src_old`. Code of various discarded implementations that perform worse than the final one.
	* TODO
* `tmp/`. Folder containing cached data files.
* `clear.sh`. Clear all execution-derived data files in `out/` and `tmp/`.
* `requirements.txt`. Standard Python-PIP requirements file with required packages and versions.

## Environment

The required environment for running the code and reproducing the results is a computer with a valid installation of Python 3. More specifically, [Python 3.6.3](https://docs.python.org/3.6/) is used.

Besides that (and the built-in Python libraries), the following packages are used and have to be installed:

* [NumPy 1.13.3](http://www.numpy.org).
* [Pandas 0.21.1](https://pandas.pydata.org)
* [Suprise (scikit-surprise) 1.0.4](http://surpriselib.com)

The best way to reproduce the complete environment would be creating a [virtualenv](https://virtualenv.pypa.io/en/stable/) using [virtualenvwrapper](https://virtualenvwrapper.readthedocs.io/en/latest/) with the following commands:

```
mkvirtualenv ml -p python3
workon ml
```

And then, inside the root folder, install the packages specified on `requirements.txt`:

```
pip install -r requirements.txt
```

## Running

All the following commands assume that the user is working inside the environment created in the previous step.

### Main program

To run the main program, the `run.py` has to be executed from the scripts folder:

```
cd scripts/
python run.py
```

There are no command line arguments. It will look for a `train.csv` file located on `../data/` for training data and the same for `test.csv` and testing. Outputs will be generated in the `../out/` folder.

The program will output the following progress updates through the standard output:

```
PARSING TRAIN
LOADING PCKL FILE FROM ../tmp/ys_train.pckl
LOADING PCKL FILE FROM ../tmp/x_train.pckl
LOADING PCKL FILE FROM ../tmp/ids_train.pckl
PARSING TEST
LOADING PCKL FILE FROM ../tmp/x_test.pckl
LOADING PCKL FILE FROM ../tmp/ids_test.pckl
FILTERING DATA
BUILDING POLYNOMIALS
LEARNING MODEL BY LEAST SQUARES
PREDICTING VALUES
EXPORTING CSV
```

### Test

To run different validations, the `test.py` has to be executed from the scrips folder:

```
cd scripts/
python3 run.py
```
There are three command line arguments: 

```
usage: test.py [-h] [--filter FILTER] [--method METHOD] [--X X]

optional arguments:
  -h, --help       show this help message and exit
  --filter FILTER  choose the pre-processing data way 0: raw data 1:
                   standardize 2: standardize + discard outliers 3: remove
                   features with -999 values 4: remove data points with -999
                   values
  --method METHOD  choose between Least Squares ('LS') or Regularized Logistic
                   Regression ('RLR')
  --X X            choose between X-validation among different degrees ('BD')
                   or lambdas ('BL'). For simple X-validation with the default
                   hyperparameterssetting, choose 'XV'
```


Outputs will be generated in the `../out/` folder. The program will output the following progress updates through the standard output:

```
PARSING TRAIN
PARSING TEST
...
LOSS_TEST X DEGREE N
LOSS_TEST X DEGREE N
LOSS_TEST X DEGREE N
...
MIN TEST ERROR: Y FOR M DEGREE
```

### Check

To run the main program, the `check.py` has to be executed from the scripts folder:

```
cd scripts/
python3 check.py
```

There are no command line arguments. It will look for a `train.csv` file located on `../data/` for training data. No file outputs will be generated.

The program will output the following progress updates through the standard output:

```
PARSING TRAIN
PARSING TEST
FILTERING DATA
BUILDING POLYNOMIALS
LEARNING MODEL BY LEAST SQUARES
X
LEARNING MODEL BY GRADIENT DESCENT
X
LEARNING MODEL BY STOCHASTIC GRADIENT DESCENT
X
LEAST SQUARES
W : [...]
MSE: X
GRADIENT DESCENT
W : [...]
MSE: X
STOCHASTIC GRADIENT DESCENT
W : [...]
MSE: X

```

### Data caching

Loading big datasets from `.csv` files is a slow task (roughly 15-20 seconds in a modern laptop), especially when developing. To solve this problem, a caching mechanism has been implemented: instead of loading raw `.csv`, the program loads `pickle`-encoded Python objects.

To do so, the main program detects if the required `.pckl` files are available in the `tmp/` folder. If so, it imports the objects into the variables used by the rest of the script. Otherwise it imports them from the `.csv` files and then dumps those objects to the `.pckl` files.
